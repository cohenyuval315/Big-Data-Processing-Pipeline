{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from binance import Client\n",
    "from datetime import timedelta,datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from kafka.producer import MyProducer\n",
    "import json\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24 Jul 2023'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE = \"1 jan 2017\" # after successful fetching, change to END_DATE\n",
    "END_DATE = datetime.today().strftime(\"%d %b %Y\")\n",
    "END_DATE #24 July 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.today()\n",
    "start_date = datetime.strptime(START_DATE, \"%d %b %Y\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [\n",
    "    (\"1m\",1000 * 60),    \n",
    "    (\"3m\",1000 * 60 * 3),\n",
    "    (\"5m\",1000 * 60 * 5),\n",
    "    (\"15m\",1000 * 60 * 15),\n",
    "    (\"30m\",1000 * 60 * 30),\n",
    "    (\"1h\",1000 * 60 * 60 * 1),    \n",
    "    (\"2h\",1000 * 60 * 60 * 2),\n",
    "    (\"4h\",1000 * 60 * 60* 4),\n",
    "    (\"6h\",1000 * 60 * 60 * 6),\n",
    "    (\"8h\",1000 * 60 * 60 * 8),\n",
    "    (\"12h\",1000 * 60 * 60 * 12),\n",
    "    (\"1d\",1000 * 60 * 60 * 8),    \n",
    "    (\"3d\",1000 * 60 * 60 * 24 * 3),\n",
    "    (\"1w\",1000 * 60 * 60 * 24 * 3),\n",
    "]\n",
    "symbols = [\"BTCUSDT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get('BINANCE_API_KEY')\n",
    "SECRET_KEY = os.environ.get('BINANCE_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_KAFKA = True\n",
    "KAFKA_PARTITIONER_SEPERATOR = \"|\"\n",
    "KAFKA_TOPIC_PREFIX = \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_CSV = True\n",
    "BASE_DIR = \"./../../../data/_raw\"\n",
    "OVERWRITE = True\n",
    "MAX_BATCH_SIZE = 256\n",
    "ROWS_PER_FILE = MAX_BATCH_SIZE * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(api_key=API_KEY,api_secret=SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_intervals(start_time, end_time, ms):\n",
    "    interval_delta = timedelta(milliseconds=ms)\n",
    "    current_time = start_time\n",
    "    count = 0\n",
    "    while current_time <= end_time:\n",
    "        current_time += interval_delta\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_file(base_dir,symbol,interval,file_idx,data,override=False): \n",
    "    file_name = f\"RAW_{symbol}_{interval}_{file_idx}.csv\"\n",
    "    file_path = os.path.join(base_dir,symbol,interval,file_name)\n",
    "    if not os.path.exists(file_path) or (override is True and os.path.exists(file_path)):\n",
    "        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])\n",
    "        # df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df.to_csv(file_path, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_topic(symbol:str,interval:str,prefix:str=\"raw\"):\n",
    "    topic = f'{prefix}_{symbol.lower()}_{interval.lower()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def data_to_csv(data,symbol,interval,total):\n",
    "    os.makedirs(os.path.join(BASE_DIR), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_DIR,symbol), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE_DIR,symbol,interval), exist_ok=True) \n",
    "    num_rows_per_file = ROWS_PER_FILE\n",
    "    file_rows = []\n",
    "    file_num = 0\n",
    "    for idx,item in tqdm(enumerate(data),total=total):\n",
    "        file_rows.append(item)\n",
    "        if idx != 0 and idx % num_rows_per_file == 0:\n",
    "            create_csv_file(base_dir=BASE_DIR,symbol=symbol,interval=interval,file_idx=file_num,data=file_rows,override=OVERWRITE)\n",
    "            file_rows = []\n",
    "            file_num += 1\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "    if len(file_rows) > 0:\n",
    "        create_csv_file(base_dir=BASE_DIR,symbol=symbol,interval=interval,file_idx=file_num,data=file_rows,override=OVERWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replication-factor 3 — partitions 3\n",
    "# Hash(Key) % Number of partitions -> Partition number\n",
    "# kafka-console-producer.bat — topic atm1 — bootstrap-server localhost:9092 — property “parse.key=true” — property “key.separator=:”\n",
    "class TimestampPartitioner(object):\n",
    "    def __init__(self,seperator = '|') -> None:\n",
    "        self.seperator = seperator\n",
    "        \n",
    "    def __call__(self, topic, key, partitions, *args, **kwargs):\n",
    "        timestamp_key = key.decode('utf-8')\n",
    "        timestamp = int(timestamp_key.split(self.seperator)[0])\n",
    "        num_partitions = len(partitions)\n",
    "        return timestamp % num_partitions\n",
    "    \n",
    "custom_partitioner = TimestampPartitioner(KAFKA_PARTITIONER_SEPERATOR)\n",
    "\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'partitioner': custom_partitioner,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interval:  1m\n",
      "3444481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 427001/3444481 [05:22<37:40, 1335.09it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def data_to_kafka(data,symbol,interval,approx_total,prefix=KAFKA_TOPIC_PREFIX,seperator=KAFKA_PARTITIONER_SEPERATOR):\n",
    "    producer = MyProducer(config=producer_config)\n",
    "    for idx,item in tqdm(enumerate(data),total=approx_total):\n",
    "        timestamp = item[\"E\"] # event timestamp\n",
    "        item_key = f\"{timestamp}{seperator}{symbol}_{interval}_{idx}\"\n",
    "        key = item_key.encode('utf-8')\n",
    "        producer.produce(json.dumps(item),key=key,topic=_make_topic(symbol,interval,prefix))\n",
    "        await asyncio.sleep(0.1)\n",
    "    \n",
    "async def fetch_historical_data(prefix=KAFKA_TOPIC_PREFIX,seperator=KAFKA_PARTITIONER_SEPERATOR):\n",
    "    for symbol in symbols:\n",
    "        for interval in intervals:\n",
    "            _interval = interval[0]\n",
    "            \n",
    "            approx = interval[1]\n",
    "            approx_total = count_intervals(start_date,end_date,approx)\n",
    "            data = client.get_historical_klines_generator(\n",
    "                symbol=symbol,\n",
    "                interval=_interval,\n",
    "                start_str=START_DATE,\n",
    "                end_str=END_DATE,\n",
    "            )\n",
    "            if TO_CSV:\n",
    "                print(\"csv - interval: \", _interval)\n",
    "                asyncio.create_task(data_to_csv(data,symbol,_interval,approx_total))\n",
    "\n",
    "            if TO_KAFKA:\n",
    "                print(\"kafka - interval: \", _interval)\n",
    "                asyncio.create_task(data_to_kafka(data,symbol,_interval,approx_total,prefix,seperator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    asyncio.run(fetch_historical_data())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
