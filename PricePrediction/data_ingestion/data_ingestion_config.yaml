to_kafka: true
data_sources:
  binance_api:
    real_time_stream:
      run: true
      streams: [
        "btcusdt@kline_1m"
      ]
      sleep_delay: 0.1
      to_kafka: true
      default_topic: "raw_data"
      print: false
      sanity_check: true
      stop_on_exception: true
      to_timeout: false
      timeout: 10

    historical_data:
      run: false
      start_date: "1 jan 2017"
      to_csv: false
      sleep_delay: 0.1
      print: false
      to_kafka: true
      timestamp_partitioner: false
      timestamp_partitioner_seperator: '|'
      default_topic: "raw_historical_data"
      symbols: [BTCUSDT]
      csv_file_prefix: RAW
      intervals: [
          ("1m",1000 * 60),    
          # ("3m",1000 * 60 * 3),
          # ("5m",1000 * 60 * 5),
          # ("15m",1000 * 60 * 15),
          # ("30m",1000 * 60 * 30),
          # ("1h",1000 * 60 * 60 * 1),    
          # ("2h",1000 * 60 * 60 * 2),
          # ("4h",1000 * 60 * 60* 4),
          # ("6h",1000 * 60 * 60 * 6),
          # ("8h",1000 * 60 * 60 * 8),
          # ("12h",1000 * 60 * 60 * 12),
          # ("1d",1000 * 60 * 60 * 8),    
          # ("3d",1000 * 60 * 60 * 24 * 3),
          # ("1w",1000 * 60 * 60 * 24 * 3),
      ]
      csv_config:
        base_dir: "./../../../data/_raw"
        overwrite: true
        max_batch_size: 256
        num_batches_in_file: 100

kafka:
  producer_config: 
    bootstrap_servers: "localhost:9092"


  
